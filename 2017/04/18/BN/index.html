<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>BN for deep learning | Fall Cloud</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="SummaryThis a paper summary for Accelerating Deep Network training by Reducing internal covariance shift. Internal covariance shift is a serious problem in deep learning training, so BN is designed">
<meta name="keywords">
<meta property="og:type" content="article">
<meta property="og:title" content="BN for deep learning">
<meta property="og:url" content="http://yoursite.com/2017/04/18/BN/index.html">
<meta property="og:site_name" content="Fall Cloud">
<meta property="og:description" content="SummaryThis a paper summary for Accelerating Deep Network training by Reducing internal covariance shift. Internal covariance shift is a serious problem in deep learning training, so BN is designed">
<meta property="og:updated_time" content="2017-04-18T18:10:51.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="BN for deep learning">
<meta name="twitter:description" content="SummaryThis a paper summary for Accelerating Deep Network training by Reducing internal covariance shift. Internal covariance shift is a serious problem in deep learning training, so BN is designed">
  
    <link rel="alternate" href="/atom.xml" title="Fall Cloud" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fall Cloud</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-BN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/18/BN/" class="article-date">
  <time datetime="2017-04-18T15:44:05.000Z" itemprop="datePublished">2017-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      BN for deep learning
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script> </p>
<hr>
<p>Summary<br>This a paper summary for <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Accelerating Deep Network training by Reducing internal covariance shift</a>. Internal covariance shift is a serious problem in deep learning training, so BN is designed to fix it. Normalizing Wx+b is better than x, it is like a hidden layer added after the linear transformation. On the one hand, we want the input distribution  in each layer among the same range, relieving us from readjusting paramters, on the other hand, we hope to keep the network’s information as much as possible,so scale after normalization can’t be ignored. BN will stabilize learning rate regardless of the input scale, improve accuracy. </p>
<p>Problem</p>
<ol>
<li>Internal covariance shift<ol>
<li>Saturated non-linearity</li>
<li>Low learning rate and careful parameter initialization</li>
</ol>
</li>
<li>Method<ol>
<li>The gradient of loss over a mini-batch is an estimation of the gradient over the training dataset</li>
<li>Computation over a batch can be more efficient than m computations for individuals.</li>
</ol>
</li>
</ol>
<p>Theory</p>
<ol>
<li>Fixed distribution of input to a sub-network is good<ol>
<li>For sub-network<ol>
<li>\(\theta_2\) doesn’t have to readjust to compensate for the changes in distribution of x</li>
</ol>
</li>
<li>For layer outside the sub-network<ol>
<li>Saturation problem and the resulting vanishing gradients(other solutions?)<ol>
<li>Relu(x,0) = max(x,0), careful initialization and small learning rate</li>
<li>Avoid getting stuck in the saturation regime</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<p>Internal covariance shift</p>
<pre><code>1. What: The change of distributions of internal nodes of a network,(or more specifically, the activations) in the course of training
2. Why: For faster training
3. How to accomplish the goal
    1. A normalization step that fixes the mean and variance of layer input
    2. Gradient flow: reduce dependence of gradient on the scale of parameters or their initial values
        1. Allow higher learning rate without the risk of divergence
        2. Without dropout: regularize the model
        3. makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes. 
4. How to implement
    1. Wrong ways
    Normalization is implemented outsize the gradient descent step.
    2. Complex ways
        1. Calculating Jacobians in term of X
        2. Costly
    3. Goal:
        1. Preserve information in the network by normalizing the activations in a training example relative to all the training data.
    4. Simplification
        0. Since normalization of each layer&apos;s input is costly and not  everywhere differentiable
            1. Notice: simply normalizing each layer may change what the layer can represent(Eg normalize the input to sigmoid may constrain it to nonlinear regime)
        1. Normalize each scalar feature independently
        2. Use mini-batch produce estimates of mean and variance of each activation
</code></pre><hr>
<p>BN Algo<br>Algo1(a mini batch of size m over K inputs): For the input K features, learn \(\gamma^{(k)},\beta^{(k)}\)<br>Algo2:(train a BN network) </p>
<ol>
<li>get inference BN network</li>
<li><p>because we want the output depend on the input, deterministically</p>
<ol>
<li>replace y with \(x=\frac{\gamma}{Var[x]+ \epsilon}x+ (\beta- \frac{\gamma E[x]}{\sqrt{Var[x]+ \epsilon}})\)</li>
</ol>
</li>
<li><p>Changes: for convolutional neural network: normalize jointly, m*pq</p>
</li>
</ol>
<hr>
<p>Experiments</p>
<ol>
<li>show the last layer’s activations over time in MNIST dataset with 3 layer fully connected layer 100 hidden units.BN is more stable</li>
<li>ImageNet classification<ol>
<li>Accelerate training</li>
<li>improve accuracy</li>
</ol>
</li>
</ol>
<hr>
<p>Additional:</p>
<ol>
<li>The network converges faster if its input is whitened<ol>
<li>Linearly transformed to have zero mean, unit variance and decorrelated</li>
</ol>
</li>
<li>Batch Normalization may lead the layer Jacobians to have singular values close to 1, which is known to be beneficial for training (Saxe et al., 2013).<br>Comment:</li>
<li>Based on the idea: Preserve information in the network by normalizing the activations in a training example relative to all the training data and output of the normalization should be deterministically, Algo2 part 2 is designed. Sometimes, statistical value, stochastic method can be applied to capture global information. Meaningful operation that can be explained in various ways is more interesting.</li>
<li>The network converges faster if its input is whitened -&gt; the following steps in this paper</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/18/BN/" data-id="cj1nveedm0000f9wqanu0t2u3" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2017/04/12/hello-world/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          Hello World
        
      </div>
    </a>
  
  
    <a href="/2017/04/18/cifar_tensorflow/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Training Cifar－10 with tensorflow</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/04/18/写博客/">写博客</a>
          </li>
        
          <li>
            <a href="/2017/04/18/cifar_tensorflow/">Training Cifar－10 with tensorflow</a>
          </li>
        
          <li>
            <a href="/2017/04/18/BN/">BN for deep learning</a>
          </li>
        
          <li>
            <a href="/2017/04/12/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Qiujing Lu<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>