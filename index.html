<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Fall Cloud</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta property="og:type" content="website">
<meta property="og:title" content="Fall Cloud">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="Fall Cloud">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Fall Cloud">
  
    <link rel="alternate" href="/atom.xml" title="Fall Cloud" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Fall Cloud</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="Flux RSS"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Rechercher"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-tensorflow安装" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/24/tensorflow安装/" class="article-date">
  <time datetime="2017-04-25T04:13:02.000Z" itemprop="datePublished">2017-04-24</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/24/tensorflow安装/">装机</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li>买了cpu，cpu冷却风扇，主板，内存条，SATA，无线模块，CD驱动，GPU，机箱，电源。感觉cpu冷却有点多余，机箱内配了3个冷却风扇。<ol>
<li>漏装standoff，主板紧贴着机箱，一边装一边觉得不科学。重装。</li>
<li>线虽然不多，但是长短粗细不一，绑了好几道还是不能看。看大家成品都是吧线放在机箱外侧挡板里的。重装。感谢坚强的主板。</li>
<li>第一次开机自检，每个debug light都会稳定亮2－3min。</li>
</ol>
</li>
<li>由于脱网，用u版制作了一个ubuntu启动盘。</li>
<li>无线模块TP-LINK 光盘附赠的是window驱动。于是利用安装系统restricted 包里的信息安装了driver，有网之后再update。<ol>
<li>查看网络：iwconfig</li>
<li>离线安装：<a href="https://askubuntu.com/questions/626642/how-to-install-broadcom-wireless-drivers-offline" target="_blank" rel="external">参考</a></li>
<li>查看网卡和芯片型号：lspci -v</li>
</ol>
</li>
<li>安装cuda相关<br><a href="https://chenrudan.github.io/blog/2016/03/06/introductionofgpusoftware.html" target="_blank" rel="external">【GPU编程系列之二】CUDA的软件层面和NVCC编译流程</a>在安装CUDA的时候，会安装三个大的组件[1]，分别是NVIDIA驱动、toolkit和samples。驱动用来控制gpu硬件，toolkit里面包括nvcc编译器、Nsight调试工具(支持Eclipse和VS，linux用cuda-gdb)、分析和调试工具和函数库。samples或者说SDK，里面包括很多样例程序包括查询设备、带宽测试等等。<ol>
<li>Download and install Toolkit8.0<ol>
<li>error: be running an X server<ol>
<li>can’t install Nvidia .run files while logged in</li>
<li>solution: <ol>
<li>ctrl+shift+F1</li>
<li>kill X server session: sudo service lightdm stop</li>
<li>sudo init 3</li>
<li>reboot after installation</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>验证samples的运行<ol>
<li>error：运行测试NVIDIA-CUDA-8.0_Samples/bin/x86_64/linux/release/deviceQuery 报错，no CUDA-capable device is detected.<ol>
<li>lspci -v : GPU 还好好活着。</li>
<li>nvcc -V: driver installed</li>
<li>路径echo 一下发现了问题，虽然导出过了，但是重开一个terminal所有的修改就消失了。<ol>
<li>没有找到how to permanently export </li>
<li>最后手动修改/etc/environment, ADD PATH: /usr/local/cuda/bin ; LD_LIBRARY_PATH = /usr/local/cuda/lib64</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li>安装cuDNNv5.1<ol>
<li>将下载的include，lib64 merge to cuda/include， cuda/lib64</li>
</ol>
</li>
</ol>
</li>
<li>安装tensorflow<ol>
<li>sudo apt-get install libcupti-dev</li>
<li>sudo apt-get install python-pip python-dev</li>
<li>pip install tensorflow-gpu</li>
</ol>
</li>
<li>运行<a href="https://github.com/smallcorgi/Faster-RCNN_TF" target="_blank" rel="external">faster－RCNN</a> 试一试<ol>
<li>error: undefined symbol:_ZN10tensorflow7strings6…..<br> 1.roi_pooling_layer gcc 编译版本不一致。<pre><code>add -D_GLIBCXX_USE_CXX11_ABI=0 to make.sh
</code></pre></li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/24/tensorflow安装/" data-id="cj1xva92l00053rvfaclcqb1d" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-写博客" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/18/写博客/" class="article-date">
  <time datetime="2017-04-18T17:07:36.000Z" itemprop="datePublished">2017-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/18/写博客/">写博客</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Markdown is a way to write content for the web,designed by John Gruber in 2004. 尽管它的文本在编辑的时候看起来很清爽简单，但是我用起来并不顺手，插图和公式都有些麻烦，也可能是因为还没有找到更适合的方法。<br>编辑器<br>Byword 不错，界面清爽，缺点是文件存储和检索不方便。用nvALT创建和保存文件，用Byword打开。用keyboard Maestro Editor编辑打开的快捷方式。<br>用Markdown写公式<br>根据<a href="http://www.jianshu.com/p/888c5eaebabd" target="_blank" rel="external">如何在markdown中插入数学公式</a><br>尝试了Google Chart的服务器，forkosh服务器,MathJax引擎,最终选了MathJax引擎。Google Chart的服务器，forkosh服务器都非常的简陋，一旦公式里面有2组（）就不能识别，forkosh服务器还会有“可爱”的大红水印。。。MathJax引擎就放便灵活多了。</p>
<ol>
<li>在Markdown 里添一行<br><code>&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</code></li>
<li>对于行间公式，<code>$$x$$</code></li>
<li>对于行内公式。<code>\\x(\\)</code><br>用Markdown插图</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/18/写博客/" data-id="cj1xva92n00063rvfkhmw5o7t" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-cifar_tensorflow" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/18/cifar_tensorflow/" class="article-date">
  <time datetime="2017-04-18T16:05:26.000Z" itemprop="datePublished">2017-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/18/cifar_tensorflow/">Training Cifar－10 with tensorflow</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <ol>
<li>install tensorflow<ol>
<li>pip install tensorflow-gpu</li>
<li>尝试 import tensorflow 发现少cudnn</li>
</ol>
</li>
<li>配置cudnn<ol>
<li>下载cudnn匹配版本<a href="https://developer.nvidia.com/rdp/cudnn-download" target="_blank" rel="external"></a></li>
<li>解压将 include 放到 /usr/local/cuda/include, lib64 放到 /usr/local/cuda/lib64</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/18/cifar_tensorflow/" data-id="cj1xva92h00023rvfqzd7si5d" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-BN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/18/BN/" class="article-date">
  <time datetime="2017-04-18T15:44:05.000Z" itemprop="datePublished">2017-04-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/18/BN/">BN for deep learning</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script> </p>
<hr>
<p>Summary<br>This a paper summary for <a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="external">Accelerating Deep Network training by Reducing internal covariance shift</a>. Internal covariance shift is a serious problem in deep learning training, so BN is designed to fix it. Normalizing Wx+b is better than x, it is like a hidden layer added after the linear transformation. On the one hand, we want the input distribution  in each layer among the same range, relieving us from readjusting paramters, on the other hand, we hope to keep the network’s information as much as possible,so scale after normalization can’t be ignored. BN will stabilize learning rate regardless of the input scale, improve accuracy. </p>
<p>Problem</p>
<ol>
<li>Internal covariance shift<ol>
<li>Saturated non-linearity</li>
<li>Low learning rate and careful parameter initialization</li>
</ol>
</li>
<li>Method<ol>
<li>The gradient of loss over a mini-batch is an estimation of the gradient over the training dataset</li>
<li>Computation over a batch can be more efficient than m computations for individuals.</li>
</ol>
</li>
</ol>
<p>Theory</p>
<ol>
<li>Fixed distribution of input to a sub-network is good<ol>
<li>For sub-network<ol>
<li>\(\theta_2\) doesn’t have to readjust to compensate for the changes in distribution of x</li>
</ol>
</li>
<li>For layer outside the sub-network<ol>
<li>Saturation problem and the resulting vanishing gradients(other solutions?)<ol>
<li>Relu(x,0) = max(x,0), careful initialization and small learning rate</li>
<li>Avoid getting stuck in the saturation regime</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
<hr>
<p>Internal covariance shift</p>
<pre><code>1. What: The change of distributions of internal nodes of a network,(or more specifically, the activations) in the course of training
2. Why: For faster training
3. How to accomplish the goal
    1. A normalization step that fixes the mean and variance of layer input
    2. Gradient flow: reduce dependence of gradient on the scale of parameters or their initial values
        1. Allow higher learning rate without the risk of divergence
        2. Without dropout: regularize the model
        3. makes it possible to use saturating nonlinearities by preventing the network from getting stuck in the saturated modes. 
4. How to implement
    1. Wrong ways
    Normalization is implemented outsize the gradient descent step.
    2. Complex ways
        1. Calculating Jacobians in term of X
        2. Costly
    3. Goal:
        1. Preserve information in the network by normalizing the activations in a training example relative to all the training data.
    4. Simplification
        0. Since normalization of each layer&apos;s input is costly and not  everywhere differentiable
            1. Notice: simply normalizing each layer may change what the layer can represent(Eg normalize the input to sigmoid may constrain it to nonlinear regime)
        1. Normalize each scalar feature independently
        2. Use mini-batch produce estimates of mean and variance of each activation
</code></pre><hr>
<p>BN Algo<br>Algo1(a mini batch of size m over K inputs): For the input K features, learn \(\gamma^{(k)},\beta^{(k)}\)<br>Algo2:(train a BN network) </p>
<ol>
<li>get inference BN network</li>
<li><p>because we want the output depend on the input, deterministically</p>
<ol>
<li>replace y with \(x=\frac{\gamma}{Var[x]+ \epsilon}x+ (\beta- \frac{\gamma E[x]}{\sqrt{Var[x]+ \epsilon}})\)</li>
</ol>
</li>
<li><p>Changes: for convolutional neural network: normalize jointly, m*pq</p>
</li>
</ol>
<hr>
<p>Experiments</p>
<ol>
<li>show the last layer’s activations over time in MNIST dataset with 3 layer fully connected layer 100 hidden units.BN is more stable</li>
<li>ImageNet classification<ol>
<li>Accelerate training</li>
<li>improve accuracy</li>
</ol>
</li>
</ol>
<hr>
<p>Additional:</p>
<ol>
<li>The network converges faster if its input is whitened<ol>
<li>Linearly transformed to have zero mean, unit variance and decorrelated</li>
</ol>
</li>
<li>Batch Normalization may lead the layer Jacobians to have singular values close to 1, which is known to be beneficial for training (Saxe et al., 2013).<br>Comment:</li>
<li>Based on the idea: Preserve information in the network by normalizing the activations in a training example relative to all the training data and output of the normalization should be deterministically, Algo2 part 2 is designed. Sometimes, statistical value, stochastic method can be applied to capture global information. Meaningful operation that can be explained in various ways is more interesting.</li>
<li>The network converges faster if its input is whitened -&gt; the following steps in this paper</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/18/BN/" data-id="cj1xva92c00003rvfht7hzbmj" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  
    <article id="post-hello-world" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/04/12/hello-world/" class="article-date">
  <time datetime="2017-04-12T23:27:55.000Z" itemprop="datePublished">2017-04-12</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2017/04/12/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="external">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="external">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="external">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo new <span class="string">"My New Post"</span></div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="external">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo server</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="external">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo generate</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="external">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$ hexo deploy</div></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="external">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/04/12/hello-world/" data-id="cj1xva92k00043rvfusq7u1bb" class="article-share-link">Partager</a>
      
      
    </footer>
  </div>
  
</article>


  

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/04/">April 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Articles récents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/04/24/tensorflow安装/">装机</a>
          </li>
        
          <li>
            <a href="/2017/04/18/写博客/">写博客</a>
          </li>
        
          <li>
            <a href="/2017/04/18/cifar_tensorflow/">Training Cifar－10 with tensorflow</a>
          </li>
        
          <li>
            <a href="/2017/04/18/BN/">BN for deep learning</a>
          </li>
        
          <li>
            <a href="/2017/04/12/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 Qiujing Lu<br>
      Propulsé by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>